{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; clear: both;\">\n",
    "    <div style=\"float: left; width: 50%;\">\n",
    "       <img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<div style=\"float: right; width: 50%;\">\n",
    "    <p style=\"margin: 0; padding-top: 22px; text-align:right;\">22.401 · Fonaments de Programació</p>\n",
    "    <p style=\"margin: 0; text-align:right;\">Grau en Ciència de Dades Aplicada</p>\n",
    "    <p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudis d'Informàtica, Multimèdia i Telecomunicació</p>\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"width:100%;\">&nbsp;</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonaments de Programació\n",
    "============================\n",
    "\n",
    "--- \n",
    "\n",
    "Unitat 7: Anàlisi de dades en Python\n",
    "-----------------------------------------------------\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruccions d'ús\n",
    "\n",
    "Aquest document és un *notebook* interactiu que intercala explicacions més aviat teòriques de conceptes de programació amb fragments de codi executables. Per aprofitar els avantatges que aporta aquest format, us recomanem que, en primer lloc, llegiu les explicacions i el codi que us proporcionem. D'aquesta manera tindreu un primer contacte amb els conceptes que hi exposem. Ara bé, **la lectura és només el principi!** Una vegada hàgiu llegit el contingut, no oblideu executar el codi proporcionat i modificar-lo per crear-ne variants que us permetin comprovar que heu entès la seva funcionalitat i explorar-ne els detalls d'implementació. Per últim, us recomanem també consultar la documentació enllaçada per explorar amb més profunditat les funcionalitats dels mòduls presentats. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruccions d'ús\n",
    "\n",
    "Aquest document és un *notebook* interactiu que intercala explicacions més aviat teòriques de conceptes de programació amb fragments de codi executables. Per aprofitar els avantatges que aporta aquest format, us recomanem que, en primer lloc, llegiu les explicacions i el codi que us proporcionem. D'aquesta manera tindreu un primer contacte amb els conceptes que hi exposem. Ara bé, **la lectura és només el principi!** Una vegada hàgiu llegit el contingut, no oblideu executar el codi proporcionat i modificar-lo per crear-ne variants que us permetin comprovar que heu entès la seva funcionalitat i explorar-ne els detalls d'implementació. Per últim, us recomanem també consultar la documentació enllaçada per explorar amb més profunditat les funcionalitats dels mòduls presentats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/drive/MyDrive/Colab_Notebooks/prog_datasci_7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducció\n",
    "\n",
    "Aquesta unitat mostra com implementar algunes de les tècniques més populars d'anàlisi de dades en Python. En primer lloc, es presenta una anàlisi exploratòria bàsica d'un conjunt de dades. A continuació, veurem com reduir la dimensionalitat del conjunt de dades, fent servir PCA. Després, implementarem un classificador i un mètode de clústering de dades, i en visualitzarem els resultats. Finalment, veurem com validar el model aprés fent servir la llibreria sklearn.\n",
    "\n",
    "A continuació s'inclou la taula de continguts, que podeu fer servir per navegar pel document:\n",
    "\n",
    "<ul style=\"list-style-type:none\">\n",
    "    <li><a href='#1-Introduccio'>1 Introducció</a></li>\n",
    "    <li><a href=\"#2-Una-petita-introduccio-a-aprenentatge-automatic\">2 Una petita introducció a l'aprenentatge automàtic</a></li>\n",
    "    <ul style=\"list-style-type:none\">\n",
    "        <li><a href=\"#2.1-Validacio-creuada\">2.1 Validació creuada</a></li>\n",
    "        <li><a href=\"#2.2-El-conjunt-de-dades-de-flors-d'Iris\">2.2 El conjunt de dades de flors d'Iris</a></li>\n",
    "    </ul>\n",
    "    <li><a href='#3-Analisi-exploratoria-de-dades'>3 Anàlisi exploratòria de dades</a></li>\n",
    "    <li><a href=\"#4-Reducció-de-la-dimensionalitat-del-dataset\">4 Reducció de la dimensionalitat del dataset</a></li>\n",
    "    <li><a href=\"#5-Classificacio\">5 Classificació</a></li>\n",
    "    <li><a href=\"#6-Clustering\">6 Clustering</a></li>\n",
    "    <li><a href=\"#7-Validacio-del-model\">7 Validació del model</a></li>    \n",
    "    <li><a href=\"#8-Exercicis-i-preguntes-teoriques\">8 Exercicis i preguntes teòriques</a></li>\n",
    "    <ul style=\"list-style-type:none\">\n",
    "        <li><a href=\"#8.1-Instruccions-importants\">8.1 Instruccions Importants</a></li>\n",
    "     </ul>  \n",
    "    <li><a href=\"#9-Bibliografia\">9 Bibliografia</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1-Introduccio'></a>\n",
    "# 1 Introducció\n",
    "\n",
    "L'anàlisi de dades i l'extracció d'informació i patrons a partir d'aquestes dades, la qual cosa es coneix com a mineria de dades, s'està convertint en peça clau en molts segments de l'economia: des d'oferir determinats descomptes a clients d'un supermercat sobre la base de les seves preferències de consum (adquirides per mitjà d'un programa de fidelització per punts, per exemple), fins a la classificació automàtica d'imatges a qualsevol xarxa social.\n",
    "\n",
    "En aquest mòdul treballarem amb llibreries que ja hem presentat als\n",
    "mòduls anteriors ([NumPy](http://www.numpy.org/),\n",
    "[pandas](http://pandas.pydata.org/) i\n",
    "[scikit-learn](http://scikit-learn.org)). Aquest Notebook conté exemples concrets de tècniques que poden\n",
    "aplicar-se per analitzar les dades. Com al mòdul anterior, és important\n",
    "destacar que s'han seleccionat únicament algunes tècniques, però, a la\n",
    "pràctica, el conjunt de tècniques que s'apliquen per a l'anàlisi de\n",
    "dades és molt més extens. A més, per a la majoria d'exemples farem servir les\n",
    "configuracions per defecte incorporades a les llibreries, però algunes\n",
    "de les funcions que provarem tenen multitud de paràmetres que podem ajustar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2-Una-petita-introduccio-a-aprenentatge-automatic'></a>\n",
    "# 2 Una petita introducció a l'aprenentatge automàtic\n",
    "\n",
    "L'aprenentatge automàtic (més conegut per la seva denominació en anglès, _machine learning_ o ML) és la branca de la intel·ligència artificial que engloba el conjunt de tècniques que permeten que els ordinadors aprenguin, és a dir, que siguin capaços de desenvolupar comportaments pels quals no han estat explícitament programats. En els últims anys, l'aprenentatge automàtic ha estat aplicat amb èxit a multitud de problemes, fent que avui dia fem servir aquest tipus de sistemes diàriament, de vegades sense ni tan sols ser-ne conscients. \n",
    "\n",
    "En el problema de l'aprenentatge es té en consideració una mostra d'_n_ observacions i, a partir d'elles, s'intenten predir propietats de dades desconegudes. Quan treballem amb dades multidimensionals, parlem de cadascun dels seus atributs com a característiques (en anglès _features_). Tornant a l'exemple del programa de descomptes a clients d'un supermercat, podríem entendre la mostra com el nombre de clients que tenen una targeta de punts (i l'han feta servir en alguna ocasió). Per a cadascun, tenim certes dades que hem pogut guardar: nombre de begudes gasoses comprades, nombre d'aliments d'origen vegetal, animal, etc. Aquestes dades que caracteritzen un determinat client és el que entendríem com a atributs. A partir d'aquests atributs, potser estaríem interessats a analitzar el consum d'aliments d'origen vegetal i no animal dels clients i llavors oferir-los determinats descomptes en productes d'origen «bio», per exemple, i augmentar la factura mensual d'aquests productes en la cadena de supermercats.\n",
    "\n",
    "Principalment, distingim dues categories d'algorismes d'aprenentatge automàtic: **supervisat** i **sense supervisió**.\n",
    "\n",
    "En l'aprenentatge automàtic supervisat, les dades contenen certs atributs que volem predir. Bé podem classificar-les en diferents categories (**classificació**) o potser ens interessi predir una variable contínua sobre la base dels atributs coneguts (**regressió**).\n",
    "\n",
    "Un exemple de classificació podria ser, mitjançant un text escanejat, assignar a cada caràcter trobat una lletra ([reconeixement d'escriptura](https://es.wikipedia.org/wiki/Reconocimiento_%C3%B3ptico_de_caracteres), un problema clàssic). Quant a un exemple de regressió, podríem voler predir l'edat d'un gos sobre la base del seu pes, altura i color i longitud del pèl.\n",
    "\n",
    "En l'aprenentatge automàtic sense supervisió l'objectiu és descobrir noves propietats de les dades d'entrada, ja sigui agrupant-les per propietats similars (_**clustering**_), descobrint la distribució que segueixen (**estimació de la densitat**) o bé projectant-les en menys o més dimensions ([PCA](https://es.wikipedia.org/wiki/An%C3%A1lisis_de_componentes_principales), visualització, etc.)\n",
    "\n",
    "Sobre _clustering_, existeixen molts casos pràctics com per exemple l'agrupació de proteïnes sobre la base del seu pes atòmic o bé la segmentació de clients en un supermercat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.1-Validacio-creuada'></a>\n",
    "\n",
    "# 2.1. Validació creuada\n",
    "\n",
    "De vegades el resultat de l'aprenentatge automàtic és un model que representa el conjunt de dades i que ens servirà per predir-ne alguna característica. Una de les tècniques que permeten avaluar la capacitat de generalització del model après és la **validació creuada**. Una ronda de validació creuada consisteix en la separació del conjunt de mostres disponibles en dos subconjunts disjunts, el conjunt d'**aprenentatge** i el conjunt de **validació** o test. El conjunt de mostres d'aprenentatge s'utilitza per entrenar el model, mentre que el conjunt de validació s'utilitza per avaluar el model. Algunes tècniques de validació creuada fan diverses rondes (amb diferents particions de les dades).\n",
    "\n",
    "Separar les mostres d'aquesta manera permet evitar problemes com l'_overfitting_, comparar la capacitat de predicció de diferents tècniques, fer una selecció de característiques, entre d'altres aplicacions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.2-El-conjunt-de-dades-de-flors-Iris'></a>\n",
    "# 2.2 El conjunt de dades de flors d'Iris\n",
    "\n",
    "En aquesta unitat treballarem amb un conjunt de dades clàssic: el conjunt de dades de **flors Iris**. A l'article de [Ronald Fisher](https://es.wikipedia.org/wiki/Ronald_Fisher) de 1936 titulat _The use of multiple measurements in taxonomic problems_, es van recopilar per tres classes de flors Iris relacionades (_Iris setosa_, _Iris virginica_ i _Iris versicolor_) quatre atributs: longitud i amplària de sèpals i pètals en cm ([anatomia d'una flor](https://es.wikipedia.org/wiki/P%C3%A9talo)). Per a cada tipus d'Iris, es van seleccionar cinquanta flors, per la qual cosa tenim una grandària d'observacions de 50 x 3 x 4 (nombre de flors x classes de flors x atributs mesurats). A continuació teniu un resum de les diferents característiques agrupades per tipus de flor (vermell per _setosa_, verd per _versicolor_, blau per _virginica_):\n",
    "\n",
    "![](img/iris_data_set.png)\n",
    "Font: Anderson's Iris data set d'Indon - Treball propi. Disponible sota la llicència CC BY-SA 3.0 via Wikimedia Commons\n",
    "\n",
    "\n",
    "Per començar, carreguem el conjunt de dades de flors d'iris:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# Carreguem el dataset d'iris:\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3-Analisi-exploratoria-de-dades'></a>\n",
    "# 3 Anàlisi exploratòria de dades\n",
    "\n",
    "En primer lloc, observarem les característiques principals de les dades\n",
    "que utilitzarem en aquest Notebook. Conèixer les dades amb les quals\n",
    "treballarem ens ajudarà després en la creació de models i la validació\n",
    "d'hipòtesis.\n",
    "\n",
    "Podem donar un cop d'ull a la descripció del _dataset_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al conjunt iris que acabem de carregar, les dades estan organitzades de\n",
    "la manera següent: cada fila és una mostra, i per a cada mostra, les\n",
    "columnes (les característiques) són: longitud del sèpal, amplada del\n",
    "sèpal, longitud del pètal i amplada del pètal.\n",
    "\n",
    "Representar visualment les dades també ens permet aproximar-nos-hi per primera \n",
    "vegada. Generarem un *scatter plot* amb els dos\n",
    "primers atributs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Importem les llibreries.\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "\n",
    "# Importem el dataset.\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Seleccionem solament els dos primers atributs.\n",
    "X = iris.data[:, :2]\n",
    "Y = iris.target\n",
    "\n",
    "# Creem la figura.\n",
    "plt.figure(1, figsize=(8, 6))\n",
    "plt.clf()\n",
    "\n",
    "# Acolorim utilitzant la categoria.\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y)\n",
    "plt.xlabel(u'Longitud del sèpal')\n",
    "_ = plt.ylabel(u'Amplada del sèpal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Després, creem un *box plot* que resumeix les dades de tots els\n",
    "atributs disponibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carreguem les dades en un dataframe de pandas.\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrem un box plot amb els quatre atributs.\n",
    "df.plot.box(vert=False, figsize=(8, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalment, mostrem histogrames per als valors de cada atribut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generem els histogrames.\n",
    "df.hist(alpha=0.7, bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4-Reduccio-de-la-dimensionsalitat-del-dataset'></a>\n",
    "# 4 Reducció de la dimensionalitat del dataset\n",
    "\n",
    "Per tal de manejar **la dimensionalitat** i evitar problemes com el sobre-ajust, s'utilitzen mètodes com l'Anàlisi de la Component Principal (**Principal Component Analysis** o **PCA**). El PCA és un mètode que s'utilitza per reduir el nombre de variables de les dades mitjançant la transformació i extracció d'una mostra de dades rellevant. D'aquesta manera es redueix la dimensió de les dades amb l’objectiu de retenir la màxima informació possible. En altres paraules, aquest mètode combina variables altament correlacionades per formar un nombre més reduït d'un conjunt artificial de variables que s'anomenen \"components principals\" que expliquen la major part de la variabilitat de les dades.\n",
    "\n",
    "Per posar un exemple, considerarem que volem reduir la dimensionalitat del seguent dataset de dues dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importem les llibreries:\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A primer cop d'ull, sembla que hi ha una forta relació lineal entre les variables x (eix horitzontal) i y (eix vertical). El PCA té l'objectiu de trobar la relació entre x i y de forma no supervisada. Això ho aconsegueix buscant una llista dels eixos principals de les dades i utilitzant aquests eixos per descriure el conjunt de dades. Amb l'estimador de PCA de skearn, podem calcular-ho de la manera següent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "\n",
    "# Amb la funció fit() podem calcular els \"components principals\" i \"variació explicada\" per cada un:\n",
    "print('Els components del PCA són :\\n ' + str(pca.components_)+ '\\n')\n",
    "\n",
    "print('La variabilitat explicada pels components és:\\n ' + str(pca.explained_variance_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Però què signifiquen aquests nombres? A continuació visualitzem-los com a vectors sobre les dades d'entrada, utilitzant els **components** per definir la direcció del vector i la **variància explicada** per definir la longitud quadrada del vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_vector(v0, v1, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2,\n",
    "                    shrinkA=0, shrinkB=0)\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "\n",
    "# mostra les dades\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    draw_vector(pca.mean_, pca.mean_ + v)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquests vectors representen els eixos principals de les dades, aquells que **expliquen la major part de la variabilitat de les dades**. En aquest cas particular l'eix més rellevant per resumir les dades és el més horitzontal (apuntant a les 8 d'un rellotge), a continuació el complementa l'eix més vertical (apuntant a les 11 d'un rellotge). Com que són complementaris, la seva direcció és sempre ortogonal. És per aquesta raó que tindrem un máxim de tants components com dimensions tingui el dataset amb els que treballem. La longitud del vector indica la importància que té aquest eix en la descripció de la distribució de les dades; més precisament, és una mesura de la variància de les dades en l'eix. \n",
    "En aquest cas, hem partit d'una dataset de 2 dimensions, la x i la y. Si vulguesim reduir la seva dimensionalitat a una sola variable sabríem que hem de transformar les dades projectant-les a l'eix més rellevant, el que correspon amb el component principal, en aquest cas sabem que seria l'eix més horitzontal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5-Classificacio'></a>\n",
    "# 5 Classificació\n",
    "\n",
    "Hi ha múltiples algorismes de classificació. Vegem un exemple de\n",
    "com fer servir un classificador *k nearest neighbors* per predir el tipus\n",
    "d'espècies d'iris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importem el classificador KNeighborsClassifier de la llibreria Sklearn.\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Importem NumPy\n",
    "import numpy as np\n",
    "\n",
    "# Seleccionem les dues primeres característiques (farem servir únicament dues característiques per\n",
    "# poder representar gràficament els resultats en 2D).\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "\n",
    "# Separem les dades (de manera aleatòria) en dos subconjunts: el d'aprenentatge i el de test.\n",
    "indices = np.random.permutation(len(iris.data))\n",
    "iris_X_train = X[indices[:-10]]\n",
    "iris_y_train = y[indices[:-10]]\n",
    "iris_X_test  = X[indices[-10:]]\n",
    "iris_y_test  = y[indices[-10:]]\n",
    "\n",
    "# Creguem el classificador.\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Entrenem el classificador.\n",
    "knn.fit(iris_X_train, iris_y_train) \n",
    "\n",
    "# Provem el classificador.\n",
    "iris_y_test_predicted = knn.predict(iris_X_test)\n",
    "\n",
    "# Mostrem els resultats de la predicció sobre el conjunt de test.\n",
    "print(\"Clases reales: \\t\\t\" + str(iris_y_test))\n",
    "print(\"Clases predichas: \\t\" + str(iris_y_test_predicted))\n",
    "print(\"Accuracy: \\t\\t\" + str(knn.score(iris_X_test, iris_y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podem visualitzar gràficament el classificador que hem après:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Importem la llibreria.\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Creem els mapes de colors que farem servir per a la representació.\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "# Calculem els limits de la visualització.\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "\n",
    "# Fem la predicció.\n",
    "h = .01\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Mostrem el resultat en una figura.\n",
    "plt.figure(1, figsize=(8, 6))\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "# Mostrem les mostres fetes servir en l'aprenentatge.\n",
    "plt.scatter(iris_X_train[:, 0], iris_X_train[:, 1], c=iris_y_train, cmap=cmap_bold)\n",
    "\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.title(\"Knn classification\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6-Clustering'></a>\n",
    "# 6 _Clustering_\n",
    "----------\n",
    "\n",
    "Com amb els algorismes de classificació, actualment hi ha multitud\n",
    "d'algorismes de _clustering_. Vegem un exemple d'utilització de\n",
    "l'algorisme *k-means*.\n",
    "\n",
    "En primer lloc, generem una visualització del conjunt de mostres. A\n",
    "continuació teniu un codi d'exemple en el qual representem la taxonomia de\n",
    "les diferents mostres (acolorim per classe d'iris) en funció de la\n",
    "longitud del sèpal (columna 0), l'amplada del sèpal (columna 1) i la\n",
    "longitud del pètal (columna 2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Carreguem les llibreries necessàries.\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import datasets\n",
    "\n",
    "# Carreguem el dataset.\n",
    "iris = datasets.load_iris()\n",
    "# Dades de la mostra\n",
    "X_iris = iris.data\n",
    "# Categories de la mostra (tres tipus d'Iris)\n",
    "Y_iris = iris.target\n",
    "\n",
    "# Creem una figura.\n",
    "fig = plt.figure(1, figsize=(8, 6))\n",
    "# De tipus 3D\n",
    "ax = Axes3D(fig, elev=-150, azim=110)\n",
    "# I representem els diferents punts, acolorint per tipus d'iris.\n",
    "ax.scatter(X_iris[:,[0]], X_iris[:,[1]], X_iris[:,[2]], c=Y_iris)\n",
    "\n",
    "# Llegendes i títols\n",
    "ax.set_title(\"Taxonomia de las 150 muestras\")\n",
    "ax.set_xlabel(\"Longitud del sepalo\")\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.set_ylabel(\"Ancho del sepalo\")\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.set_zlabel(\"Longitud del petalo\")\n",
    "_ = ax.w_zaxis.set_ticklabels([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ara farem l'experiment següent: utilitzant l'algorisme de _clustering_\n",
    "*k-means*, acolorirem utilitzant els grups que calculi l'algorisme i no\n",
    "les classes que ja coneixem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import cluster, datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X_iris = iris.data\n",
    "\n",
    "# Carreguem l'algorisme K-means i fem fit a les nostres dades:\n",
    "k_means = cluster.KMeans()\n",
    "k_means.fit(X_iris)\n",
    "\n",
    "fig = plt.figure(1, figsize=(8, 6))\n",
    "ax = Axes3D(fig, elev=-150, azim=110)\n",
    "ax.scatter(X_iris[:,[0]], X_iris[:,[1]], X_iris[:,[2]], c=k_means.labels_)\n",
    "ax.set_title(\"Taxonomia de les 150 mostres fent servir K-means\")\n",
    "ax.set_xlabel(u\"Longitud del sèpal\")\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.set_ylabel(u\"Amplada del sèpal\")\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.set_zlabel(u\"Longitud del pètal\")\n",
    "_ = ax.w_zaxis.set_ticklabels([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixeu-vos que en **cap moment s'utilitza la classe de la mostra**\n",
    "(`iris.target`) per entrenar l'algorisme ni per avaluar-l'ho! Ara estem\n",
    "utilitzant un algorisme de _clustering_, que agruparà les mostres en\n",
    "funció de les seves característiques. El resultat de l'algorisme és el\n",
    "grup al qual pertany cada mostra (però l'algorisme no intenta predir la\n",
    "classe de la mostra). Els noms dels grups generats són arbitraris (en\n",
    "aquest cas, valors enters del 0 al número de grups - 1).\n",
    "\n",
    "Forçarem que el nombre de clústers sigui igual a 3 i en representarem el\n",
    "resultat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import cluster, datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X_iris = iris.data\n",
    "\n",
    "# Carreguem l'algorisme K-means i fem fit a les nostres dades,\n",
    "# aquesta vegada forçant el nombre de clústers a tres:\n",
    "k_means = cluster.KMeans(n_clusters=3)\n",
    "k_means.fit(X_iris)\n",
    "\n",
    "fig = plt.figure(1, figsize=(8, 6))\n",
    "ax = Axes3D(fig, elev=-150, azim=110)\n",
    "ax.scatter(X_iris[:,[0]], X_iris[:,[1]], X_iris[:,[2]], c=k_means.labels_)\n",
    "ax.set_title(u\"Taxonomia de les 150 mostres forçant clústers = 3\")\n",
    "ax.set_xlabel(u\"Longitud del sèpal\")\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.set_ylabel(u\"Amplada del sèpal\")\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.set_zlabel(u\"Longitud del pètal\")\n",
    "_ = ax.w_zaxis.set_ticklabels([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordeu que utilitzant un algorisme de _clustering_ no aprenem a quina\n",
    "classe pertany cada mostra, sinó que simplement agrupem les mostres en\n",
    "grups (clústers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7-Validacio-del-model'></a>\n",
    "\n",
    "# 7 Validació del model\n",
    "\n",
    "Hem d'evitar avaluar els models amb les mateixes dades que s'han\n",
    "utilitzat per a l'aprenentatge. A l'exemple de classificació, hem\n",
    "separat les dades de manera aleatòria en dos conjunts, un per a\n",
    "l'aprenentatge i un per al test. Aquesta tècnica es coneix com a\n",
    "*holdout*. A l'exemple de la classificació hem fet servir NumPy per crear els\n",
    "dos conjunts. A la unitat 6 vam veure com fer aquest mateix procés\n",
    "usant les funcions sobre _dataframes_ que ofereix la lliberia Pandas. Ara\n",
    "veurem com podem fer-ho amb Sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importem la funció 'train_test_split'.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separem les mostres utilitzant un 20% per a test i la resta per a aprenentatge.\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=0)\n",
    "\n",
    "print(\"Total de mostres: \" + str(len(iris.data)))\n",
    "print(\"Aprenentatge: \" + str(len(X_train)), \"(\" + str(float(len(X_train))/len(iris.data)*100) + \"%)\")\n",
    "print(\"Test: \" + str(len(X_test)), \"(\" + str(float(len(X_test))/len(iris.data)*100) + \"%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "També podem fer servir altres tècniques per avaluar els models, per exemple,\n",
    "*kfold* o *leave one out*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importem la funció KFold.\n",
    "from sklearn.model_selection import KFold\n",
    "# Importem NumPy.\n",
    "import numpy as np\n",
    "\n",
    "# Dividim un conjunt de nou mostres fent servir 3-Fold i en mostrem el resultat.\n",
    "X = np.array(range(9))\n",
    "kf = KFold(n_splits=3)\n",
    "for train, test in kf.split(X):\n",
    "    print(\"%s %s\" % (X[train], X[test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importem la funció 'LeaveOneOut'.\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "# Importem NumPy\n",
    "import numpy as np\n",
    "\n",
    "# Dividim un conjunt de nou mostres fent servir 'LeaveOneOut' i mostrem el resultat.\n",
    "X = np.array(range(9))\n",
    "loo = LeaveOneOut()\n",
    "for train, test in loo.split(X):\n",
    "    print(\"%s %s\" % (X[train], X[test]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='8-Exercicis-i-preguntes-teoriques'></a>\n",
    "# 8 Exercicis i preguntes teòriques\n",
    "\n",
    "La part avaluable d'aquesta unitat consisteix en el lliurament d'un fitxer IPython Notebook amb extensió IPYNB que contindrà els diferents exercicis i les preguntes teòriques que s'han de contestar. Trobareu el fitxer (`prog_datasci_7_python_entrega.ipynb`) amb les activitats a la mateixa carpeta que aquest notebook que esteu llegint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='8.1-Instruccions-importants'></a>\n",
    "## 8.1 Instruccions importants\n",
    "\n",
    "És molt important que a l'hora de lliurar el fitxer Notebook amb les vostres activitats us assegureu que:\n",
    "\n",
    "1. Les vostres solucions siguin originals. Esperem no detectar-hi còpia directa entre estudiants.\n",
    "2. Tot el codi estigui correctament documentat. El codi sense documentar equivaldrà a un 0.\n",
    "3. El fitxer comprimit que lliureu és correcte (conté les activitats de la PAC que heu de lliurar).\n",
    "\n",
    "Per fer el lliurament, heu d'anar a la carpeta del drive Colab Notebooks, clicant botó dret a la PAC en qüestió i fent Download. D'aquesta manera us baixereu la carpeta de la PAC comprimida en zip. Aquest és l'arxiu que heu de pujar al campus de virtual de l'assignatura."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='9-Bibliografia'></a>\n",
    "# 9 Bibliografia\n",
    "\n",
    "1. Aprenentatge supervisat amb sklearn: http://scikit-learn.org/stable/supervised_learning.html\n",
    "2. Comparació de classificadors amb sklearn: http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html\n",
    "3. Aprenentatge no supervisat amb sklearn: http://scikit-learn.org/stable/unsupervised_learning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Autor original **Cristina Pérez Solà**, 2017.\n",
    "- Actualitzat per **Cristina Pérez Solà**, 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width: 30%; clear: both;\">\n",
    "    <div style=\"width:0%;\">&nbsp;</div>\n",
    "           <img src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-sa.png\">\n",
    "    </div>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
